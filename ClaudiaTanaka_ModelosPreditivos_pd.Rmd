---
title:  Projeto da Disciplina de Modelos Preditivos
author: Claudia Tanaka (claudia.tanaka@al.infnet.edu.br)
date:   "Atualizado em `r format(Sys.time(), '%d/%m/%Y')`"

output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( message=FALSE, warning=FALSE )
options(scipen=999) # "Desliga" notação científica. 

# PACOTES 
library(tidymodels)

# PRETTY DOC
library(gt)
library(patchwork)

theme_set(theme_light())
theme_update(
  panel.grid.minor = element_blank(),
  plot.title = element_text(size = 12, colour = "gray30", face = "bold"),
  plot.subtitle = element_text(face = 'italic', colour = "gray50", size = 10),
  plot.caption = element_text(colour = "gray50", hjust=0, size = 8),
  legend.title = element_blank(),
)
```

\

Nessa disciplina, aprofundamos nossos conhecimentos em modelos preditivos, tarefa que é extremamente importante para o dia-a-dia de um cientista de dados. Agora iremos validar nosso conhecimento.

Para a realização desse trabalho, será necessário utilizar a plataforma Knime e seus componentes para uso de algoritmos de Machine Learning.

Escolha uma base de dados para realizar esse projeto, onde você utilizará algoritmos de classificação. Essa base necessita ter 4 (ou mais) variáveis de interesse e 2 (apenas 2!) classes (rótulos). Caso você tenha dificuldade para escolher uma base, o(a) professor(a) da disciplina irá designar uma para você.

\

# Knime

**No relatório final, anexe um printscreen evidenciando que o Knime está funcionando com os componentes**

![Printscreen do Knime instalado na minha máquina](.imgs/knime1.png)

\

# Base escolhida

**Explique a motivação de uso da base escolhida.**

O objetivo deste projeto é avaliar a propensão ou risco de rotatividade dos clientes de uma empresa de telecomunicações. Isto é fundamental para o negócio de telecomunicações porque muitas vezes é mais caro adquirir novos clientes do que manter os existentes.

Especificamente, nosso projeto fará uma modelagem para a classificação dos clientes entre contratos cancelados ou não cancelados, gerando um modelo final que ajudará a empresa a identificar clientes altamente propensos ao cancelamento de forma que sua área comercial possa atuar especificamente sobre estes clientes.

Adicionalmente, gostaríamos de poder reportar a importância relativa das variáveis explicativas do modelo final para identificar os fatores mais influenciadores do risco de cancelamento do cliente.

\
Este projeto é baseado em um [conjunto de dados da IBM no Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).

```{r}
df0 <- read.csv("_datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv") |> 
  as_tibble() |> 
  mutate(SeniorCitizen = if_else(SeniorCitizen==0, "No", "Yes")) |> 
  mutate(across(where(is.character), as.factor)) |> 
  # Ordena para que a classificação de referência seja "Yes"
  mutate(Churn = forcats::fct_rev(Churn))

dim(df0)
```

Só há lacunas nos dados de TotalCharges

```{r}
colSums(is.na(df0)) |> as.data.frame() |> rownames_to_column() |> 
  rename(coluna=1,nulos=2) |> 
  filter(nulos>0) |> arrange(desc(nulos))
```

Como são apenas 11 observações entre 7.043, optamos por não aplicar uma metodologia de imputação, mas apenas excluir estas observações da base de dados.

```{r}
df1 <- df0 |> na.omit()
```

\

Todos os `customerID` são distintos.

```{r}
n_distinct(df0$customerID)
```

\

Só há lacunas nos dados de TotalCharges

```{r}
colSums(is.na(df0)) |> as.data.frame() |> rownames_to_column() |> 
  rename(coluna=1,nulos=2) |> 
  filter(nulos>0) |> arrange(desc(nulos))
```

Como são apenas 11 observações entre 7.043, optamos por não aplicar uma metodologia de imputação, mas apenas excluir estas observações da base de dados.

```{r}
df1 <- df0 |> na.omit()
```

\

# Variáveis

**Descreva as variáveis presentes na base. Quais são as variáveis? Quais são os tipos de variáveis (discreta, categórica, contínua)? Quais são as médias e desvios padrões?**

Cada linha representa um cliente, cada coluna contém os seguintes atributos do cliente.

Este conjunto de dados inclui as seguintes 20 variáveis explicativas (features):

-   [dados demográficos]{.underline}: Gender (gênero), SeniorCitizen (flag de idoso), Partner (se tem parceiro), Dependents (se tem dependentes)

-   [serviços assinados]{.underline}: PhoneService, MultipleLine, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies

-   [informações da conta do cliente]{.underline}: CustomerID, Contract, PaperlessBilling (recebe a fatura online), PaymentMethod (método de pagamento), MonthlyCharges (mensalidade), TotalCharges (total incluindo mensalidade), Tenure (há quantos meses é cliente)

O alvo é `Churn`, que é um flag binário sim ou não.

```{r}
glimpse(df1)
```

Todas as variáveis são categóricas, exceto a variável numérica discreta `Tenure` (há quantos meses é cliente) e as duas variáveis numéricas contínuas: (1) mensalidade (`MonthlyCharges`); e (2) valor total incluindo mensalidade (`TotalCharges`).

\

Estatísticas descritivas das variáveis numéricas:

```{r}
summary(df1 |> select(is.numeric))
```

\

Nenhuma das variáveis numéricas parece ter distribuição normal e nem valores extremos.

```{r 3-1, echo=FALSE, fig.cap="*Histograma das variáveis numéricas*", fig.asp=1}
df1 |> select(customerID, is.numeric) |> 
  pivot_longer(is.numeric) |> 
  mutate(name = factor(name, levels=c("tenure", "MonthlyCharges", "TotalCharges"),
                       labels=c("Tenure (months)", "Monthly charges", "Total charges"))) |>
  mutate(mean = mean(value), 
         median = median(value), 
         q1 = quantile(value, 0.25), 
         q3 = quantile(value, 0.75), 
         .by=name) |>
  ggplot(aes(value)) +
  geom_histogram(bins=50, color="white", fill="cadetblue") +
  geom_vline(aes(xintercept=mean, color="média"), linewidth=1, show.legend=T) +
  geom_vline(aes(xintercept=median, color = "mediana"), linewidth=1, show.legend=T) +
  geom_vline(aes(xintercept=q1, color="q1/q3"), linewidth=1, show.legend=T) +
  geom_vline(aes(xintercept=q3, color="q1/q3"), linewidth=1, show.legend=T) +
  scale_color_manual(name="", values=c("média"="black", "mediana"="blue", "q1/q3"="red")) +
  facet_wrap(.~name, scales="free", ncol=1) +
  theme(legend.position="top") +
  labs(title = "Histograma das variáveis numéricas",
       subtitle = "Distribuição das variáveis numéricas do dataset",
       x = NULL, y = "Frequência")
```

\

XXXX ===\> **Categorização das variáveis numéricas?**

\

# X Validação cruzada e EDA

**Em relação à base escolhida:**

## Validação cruzada {#validacao-cruzada}

**(1) Você irá comparar alguns modelos para prever as classes. Descreva como a validação cruzada pode ser usada para comparar modelos de maneira justa. Descreva o procedimento e como uma métrica final é calculada.**

O [Capítulo 10. Resampling for Evaluating Performance](https://www.tmwr.org/resampling#resampling) do livro Tidy Modeling with R ([Kuhn e Silge 2023]()#references) explica os fundamentos da validação cruzada.

Algumas técnicas de modelagem (como regressão linear/logística, análise discriminante e outros) são considerados modelos de alto viés pois não são altamente adaptáveis aos tipos de padrões que podem ser encontrados nos dados. Porém muitos modelos "*black-box*" (de difícil interpretação, como redes neurais, random forest) têm baixo viés, o que significa que podem reproduzir relacionamentos altamente complexos de vários tipos de padrões encontrados em um conjunto de dados.

Para um modelo de baixo viés, o alto grau de capacidade preditiva pode resultar no modelo reproduzir quase exatamente os dados do conjunto de treinamento. Um modelo de baixo viés treinado em certo conjunto de dados sempre fornecerá previsões perfeitas para estes dados de treinamento, mas pode não funcionar tão bem para novas observações. Isso é conhecido como *overfitting*. Testar as projeções no próprio conjunto de treinamento sempre resulta em uma estimativa artificialmente otimista de performance e aumenta as chances que o modelo não tenha uma boa performance em novas observações.

Uma solução popular para evitar a precisão superotimista das projeções (*overfitting*) é avaliar o desempenho não nos dados usados para construir o modelo, mas sim em uma amostra dos dados reservada à parte à qual o modelo não seja exposto durante sua construção – ou seja, uma separação dos dados em subconjunto de treinamento e subconjunto de teste.

Algumas técnicas de modelagem (como regressão linear/logística, análise discriminante e outros) são considerados modelos de alto viés pois não são altamente adaptáveis aos tipos de padrões que podem ser encontrados nos dados. Porém muitos modelos "*black-box*" (de difícil interpretação, como redes neurais, random forest) têm baixo viés, o que significa que podem reproduzir relacionamentos altamente complexos de vários tipos de padrões encontrados em um conjunto de dados.

Para um modelo de baixo viés, o alto grau de capacidade preditiva pode resultar no modelo reproduzir quase exatamente os dados do conjunto de treinamento. Um modelo de baixo viés treinado em certo conjunto de dados sempre fornecerá previsões perfeitas para estes dados de treinamento, mas pode não funcionar tão bem para novas observações. Isso é conhecido como *overfitting*. Testar as projeções no próprio conjunto de treinamento sempre resulta em uma estimativa artificialmente otimista de performance e aumenta as chances que o modelo não tenha uma boa performance em novas observações.

Uma solução popular para evitar a precisão superotimista das projeções (*overfitting*) é avaliar o desempenho não nos dados usados para construir o modelo, mas sim em uma amostra dos dados reservada à parte à qual o modelo não seja exposto durante sua construção – ou seja, uma separação dos dados em subconjunto de treinamento e subconjunto de teste.

A criação de uma amostra reservada pode ser obtida de várias maneiras, a mais comumente usada sendo a partição aleatória da amostra em conjuntos de treinamento e teste por métodos de reamostragem. Métodos de reamostragem são sistemas de simulação empírica que emulam o processo de usar alguns dados para treinamento e dados diferentes para teste. A maioria dos métodos de reamostragem são iterativos, o que significa que esse processo é repetido várias vezes. O diagrama na Figura a seguir ilustra como os métodos de reamostragem geralmente operam.

A criação de uma amostra reservada pode ser obtida de várias maneiras, a mais comumente usada sendo a partição aleatória da amostra em conjuntos de treinamento e teste por métodos de reamostragem. Métodos de reamostragem são sistemas de simulação empírica que emulam o processo de usar alguns dados para treinamento e dados diferentes para teste. A maioria dos métodos de reamostragem são iterativos, o que significa que esse processo é repetido várias vezes. O diagrama na Figura a seguir ilustra como os métodos de reamostragem geralmente operam.

![Figura 4.1 Esquema de particionamento de dados com reamostragem (Khun e Silge 2023).](.imgs/Resampling.png){#fig4-1 width="500"}

\
A reamostragem é feita somente no conjunto de treinamento, como se vê na Figura [4.1](#fig4-1). O conjunto de teste não é utilizado. Para cada iteração da reamostragem, os dados são reparticionados em duas sub-amostras:

-   O modelo é treinado com o dataset de [análise]{.underline} (*Analysis*)

-   O modelo é avaliado sobre o dataset de [avaliação]{.underline} (*Assessment*)

Essas duas sub-amostras são análogas aos conjuntos de treinamento e teste. A terminologia de [análise]{.underline} e [avaliação]{.underline} visa evitar confusão com a divisão inicial dos dados em treinamento e teste. Esses conjuntos de reamostragem são mutuamente exclusivos. O esquema de particionamento usado para criar os conjuntos de análise e avaliação é geralmente a característica definidora do método de reamostragem

Suponha que sejam feitas 20 iterações de reamostragem. Isso significa que 20 modelos separados são ajustados nos conjuntos de análise e os conjuntos de avaliação correspondentes produzem 20 conjuntos de estatísticas de desempenho. A estimativa final de desempenho para o modelo seria a média das 20 estatísticas de desempenho. Essa média tem potencial de generalização muito alto e é muito superior a estimativas feitas sem particionamento dos dados.

Um método de reamostragem muito utilizado, especialmente com amostras pequenas de dados, é a validação cruzada. Embora haja uma série de variações, o método de validação cruzada mais comum é a validação cruzada [*V-fold*]{.underline}. Os dados são particionados aleatoriamente em $V$ sub-amostras de tamanho aproximadamente igual (as sub-amostras são chamadas de *"folds"*).

A título de ilustração, $V=3$ é mostrado na Figura [4.2](#fig4-2) para um conjunto de dados de treinamento de 30 observações com alocações aleatórias dos folds. O número dentro dos símbolos identifica a observação. A cor e formato dos símbolos representa a sub-amostra ou fold escolhido aleatoriamente.

![Figura 4.2 Esquema de validação cruzada 3-fold (Khun e Silge 2023).](.imgs/CrossValidationExample.png){#fig4-2 width="350"}

\

Para validação cruzada 3-fold, as três iterações de reamostragem são ilustradas na Figura [4.3](#fig4-3). Para cada iteração, um fold é mantido para teste e os folds restantes são usados para treinar o modelo. Esse processo continua para cada fold, de modo que três modelos são gerados que produzem três conjuntos de estatísticas de performance.

![Figura 4.3 Esquema de validação cruzada 3-fold (Khun e Silge 2023).](.imgs/CrossValidationAplicationExample.png){#fig4-3 width="400"}

\

Quando $V = 3$, os conjuntos de análise são 2/3 do conjunto de treinamento e cada conjunto de avaliação é um 1/3. A estimativa final de reamostragem calcula a média das estatísticas de performance de cada uma das sub-amostras. Usar $V = 3$ é bom para ilustrar a mecânica da validação cruzada, mas é ruim na prática porque é muito pequeno para gerar estimativas confiáveis. O Teorema do Limite Central sugere que a média de várias estatísticas de desempenho de modelos ajustados em diferentes sub-amostras de dados tende a ter uma distribuição normal. Isso significa que quanto mais sub-amostras forem usadas, mais confiável será a estimativa da média.

Na prática, os valores de V são mais frequentemente $5$ ou $10$; geralmente prefere-se [**validação cruzada 10-fold**]{.underline} como padrão porque a parcela de observações usada na análise ou treino será grande o suficiente para bons resultados na maioria dos casos.

Há variações na aplicação de validações cruzadas, como por exemplo a [validação cruzada com repetição]{.underline}, que cria $R$ repetições de uma validação cruzada V-fold e a [validação cruzada "*leave-one-out"*]{.underline} (LOOCV), que é um caso especial de validação cruzada V-fold com $V = n$, onde $n$ modelos são treinados com $n-1$ observações e cada modelo projeta a observação excluida (técnica utilizada quase exclusivamente quando há amostras muito pequenas de dados). A [validação cruzada de Monte Carlo]{.underline} é uma variação em que a proporção dos dados é determinada aleatoriamente a cada fold. Isso resulta em conjuntos de avaliação que não são mutuamente exclusivos.

\

## XXX EDA

**(2) A base se encontra com as classes balanceadas? Cite uma maneira resolver no caso das classes estarem desbalanceadas.**

Neste conjunto de dados de mais de 7.000 clientes, 26% deles saíram no último mês.

```{r}

```

\

# Técnicas de modelagem

**Qual a diferença entre uma regressão linear e a regressão logística?**

Em resumo, a regressão linear é um método de modelagem de dados utilizado para prever uma variável contínua a partir de uma relação linear estimada entre uma variável target e uma ou mais variáveis explicativas. A regressão logística é um caso específico de um modelo linear generalizado (*generalized linear model* -- GLM) desenvolvido para estender a aplicação da regressão linear a um contexto em que se deseje prever uma variável categórica binária (sim/não, 0/1, evento/não evento).

Assim, apesar de ambos serem modelos lineares, a principal diferença entre os dois métodos é que a regressão linear é utilizada para prever variáveis numéricas contínuas, enquanto a regressão logística é utilizada para prever variáveis categóricas binárias.

\
Mais especificamente, com base nos capítulos sobre Regressão Linear simples, Regressão Linear multivariável e Regressão Logística do livro *Practical Statistics for Data Scientists* de [Bruce et al (2020)](#references):

A [**regressão linear simples**]{.underline} @ref(eq:reg-linear) fornece um modelo da relação entre a magnitude de uma variável e a de uma segunda – por exemplo, à medida que X aumenta, Y também aumenta. Ou à medida que X aumenta, Y diminui. Coeficiente de correlação é outra forma de medir como duas variáveis estão relacionadas. A diferença é que enquanto a correlação mede a força de uma associação entre duas variáveis, a regressão quantifica a natureza da relação.

A regressão linear simples @ref(eq:reg-linear) estima quanto Y mudará quando Y mudar em um determinado valor. Com o coeficiente de correlação, as variáveis X e Y são intercambiáveis. Com a regressão, tentamos prever a variável Y a partir de X usando uma relação linear (ou seja, uma linha):

$$
\begin{equation}
Y = \beta_0 + \beta_1 X
(\#eq:reg-linear)
\end{equation}
$$

O símbolo $\beta_0$ é a constante que determina a interseção com o eixo Y se X fosse igual a zero, e o símbolo $\beta_1$ é o coeficiente da inclinação de X. A variável Y é conhecida como variável de saída, variável *target* ou variável dependente, pois depende de X. A variável X é conhecida como variável explicativa, *feature* ou variável independente.

Considerando o gráfico de dispersão na Figura \@ref(fig:5-1) que mostra a largura da pétala de um tipo de flor (`Petal.Width`) versus o comprimento da sua pétala, pode-se visualizar como a largura está relacionada ao comprimento das pétalas. Parece que quanto maior o comprimento, maior a largura.

```{r 5-1, echo=FALSE, fig.cap="*Gráfico de dispersão Sepal.Length x Sepal.Width*", fig.width=5, fig.asp=.7}
iris |> 
  filter(Species == "versicolor") |>
  ggplot(aes(x=Petal.Length, y=Petal.Width)) +
  geom_point() +
  labs(title = "Gráfico de dispersão Sepal.Length x Sepal.Width",
       subtitle = "Iris dataset")
```

\

A regressão linear simples tenta encontrar a “melhor” linha reta ou a linha reta mais ajustada para prever a variável target Y=`Petal.Width` em função da feature X=`Petal.Length`:

$$
Petal.Width = \beta_0 + \beta_1 Petal.Length
$$\

A linha de regressão deste modelo é exibida na Figura \@ref(fig:5-2).

```{r 5-2, echo=FALSE, fig.cap="*Gráfico de dispersão Sepal.Length x Sepal.Width com regressão linear simples*", fig.width=5, fig.asp=.7}
iris |> 
  filter(Species == "versicolor") |>
  ggplot(aes(x=Petal.Length, y=Petal.Width)) +
  geom_smooth(method="lm", se=FALSE, formula="y~x") +
  geom_point() +
  labs(title = "Gráfico de dispersão Sepal.Length x Sepal.Width",
       subtitle = "Iris dataset")
```

\

É importante notar que a regressão apenas estima a variável target em função da feature explicativa com base nos dados disponíveis. Não há na regressão inferência sobre relação de causa/efeito entre as variáveis (se uma causa a outra ou vice-versa ou se apenas ocorrem juntas).

Como se vê na figura, em geral os dados não caem exatamente na linha reta que foi estimada pela regressão. Então a equação dessa linha de regressão deve incluir um termo de erro explícito que representa essas diferenças conhecidas como resíduos. Além disso, entre os estatísticos convenciona-se denotar os valores ajustados ou valores estimados por $\hat{Y}$ (Y com um chapéu), assim como nos coeficientes e no resíduo. Isso indica que os coeficientes e o resíduo também são estimados (não conhecidos) e essa estimativa inclui incerteza, enquanto o valor verdadeiro (sem o chapéu) seria exato:

$$
\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1} X_i + \hat{\epsilon}_i
$$

A linha de regressão é ajustada de modo a minimizar a soma dos quadrados dos resíduos, ou seja, a diferença entre os valores reais e os valores previstos. A linha de regressão é a estimativa que minimiza a soma dos valores residuais quadrados, também chamada de soma residual dos quadrados (residual sum of squares) ou RSS:

$$
\begin{equation}
RSS = \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2 = \sum_{i=1}^{n} (Y_i - \hat{\beta_0} - \hat{\beta_1} X_i)^2
(\#eq:rss)
\end{equation}
$$

As estimativas de $\beta_0$ e $\beta_1$ são os valores que minimizam o RSS \@ref(eq:rss). O método de minimizar a soma dos resíduos quadrados RSS é denominado regressão de mínimos quadrados ou regressão de mínimos quadrados ordinários (OLS - ordinary least squares). É frequentemente atribuído a Carl Friedrich Gauss, o matemático alemão, mas foi publicado pela primeira vez pelo matemático francês Adrien-Marie Legendre em 1805.

\
Quando existem várias variáveis explicativas, a equação é simplesmente estendida para acomodá-los em um regressão linear múltipla:

$$
\begin{equation}
\hat{Y} = \hat{\beta_0} + \hat{\beta_1} X_1 + \hat{\beta_2} X_2 + \ldots + \hat{\beta_q} X_q + \epsilon
(\#eq:reg-linear-multiple)
\end{equation}
$$

Em vez de uma linha, tem-se um modelo linear – a relação entre cada coeficiente e sua variável (feature) é linear. Essa equação é conhecida como regressão linear múltipla ou [*multiple linear regression*]{.unerline} \@ref(eq:reg-linear-multiple). Todos os outros conceitos da regressão linear simples, como o ajuste por mínimos quadrados e a definição de valores ajustados e resíduos, estendem-se à configuração de regressão linear múltipla.

\
A [**regressão logística**]{.underline} é análoga à regressão linear múltipla  \@ref(eq:reg-linear-multiple), exceto que o resultado é [binário]{.underline}. Várias transformações são empregadas para converter o problema em algo no qual um modelo linear possa ser ajustado. A regressão logística é uma abordagem de modelo estruturado, em vez de uma abordagem centrada em dados. Devido à sua velocidade de processamento com menor custo computacional e sua rapidez na classificação de novos dados, é um método bastante adotado.

Os principais ingredientes para a regressão logística são a função de resposta logística ou *logistic response function* \@ref(eq:logistic-response) e a função log-odds ou logit \@ref(eq:logit), no qual mapeia-se uma probabilidade (entre 0 e 1) para uma escala mais ampla, adequada para modelagem linear.

O primeiro passo é pensar na variável de resultado não como um rótulo binário, mas como a probabilidade $p$ de que o rótulo seja “1”. Ingenuamente, podemos cair na tentação de modelar $p$ como uma função linear das variáveis explicativas:

$$
p = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_q x_q
$$

No entanto, ajustar esta função não garante que $p$ resulte em um número entre 0 e 1, como tem que acontecer com uma probabilidade. Então, é necessário modelar $p$ aplicando uma *logistic response function* \@ref(eq:logistic-response) às variáveis explicativas:

$$
\begin{equation}
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_q x_q)}}
(\#eq:logistic-response)
\end{equation}
$$

Essa transformação garante que $p$ permaneça entre 0 e 1.

Para tirar a expressão exponencial do denominador, pode-se expressar essa equação em termos de "chances" ou [*odds*]{.underline} em vez de probabilidade.

A chance, conceito familiar aos apostadores do mundo todo, é a razão entre “sucessos” (1) e “fracassos” (0). Na linguagem de probabilidade, "chances" são a probabilidade de um evento ocorrer dividida pela probabilidade de o evento não ocorrer. Por exemplo, se a probabilidade de um cavalo ganhar for $0,5$, a probabilidade de “não ganhar” é $(1 – 0,5) = 0,5$ e as chances são de 1 pra 1, então:

$$
Odds(Y=1) = \frac{p}{1-p}
$$

É possível obter a probabilidade a partir das chances usando o inverso da função de chances:

$$
p = \frac{Odds(Y=1)}{1 + Odds(Y=1)}
$$

Combinando isso com a *logistic response function* \@ref(eq:logistic-response) vista anteriormente, tem-se que:

$$
Odds(Y=1) = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_q x_q}
$$

Finalmente, tirando o logaritmo natural de ambos os lados, obtém-se uma equação que é uma função linear das variáveis explicativas:

$$
\begin{equation}
log(Odds(Y=1)) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_q x_q
(\#eq:logit)
\end{equation}
$$

A função *log-odds*, também conhecida como função logit \@ref(eq:logit), mapeia a probabilidade $p$ de $(0,1)$ para qualquer valor $(-\infty, +\infty)$. Assim o círculo de transformação se fecha, aplicando um modelo linear para prever uma probabilidade, que por sua vez pode ser mapeado para um rótulo de classe aplicando-se uma regra de corte – qualquer observação com uma probabilidade maior que o ponto de corte é classificado como 1.

```{r 5-3, echo=FALSE, fig.cap="*Gráfico de função logit que coloca uma probabilidade em uma escala aplicável a um modelo linear.*", fig.width=5, fig.asp=.7}
# Plot logit(p) x p
ggplot(data=tibble(x=seq(0.01, 0.99, by=0.01), 
                   y=log(seq(0.01, 0.99, by=0.01)/(1-seq(0.01, 0.99, by=0.01)))),
       aes(x=x, y=y)) +
  geom_line(color="blue") +
  labs(x="p", y="logit(p)")
```

\

A resposta na função de regressão logística é o log das probabilidades de um resultado binário de 1. O resultado desejado, porém, é apenas o resultado binário e não o log das probabilidades, portanto são necessários métodos estatísticos adicionais para ajustar o resultado da equação a um limite de corte.

\

# XXX Modelos de Classificação

**Com a base escolhida:**

## Etapas de modelagem

**Descreva as etapas necessárias para criar um modelo de classificação eficiente.**

[Kuhn e Silge (2023)](#references) propõem uma descrição do processo genérico de modelagem preditiva que pode ser dividido nos seguintes passos:

1.  Definir o objetivo
2.  Analisar os dados (Exploratory Data Analysis - EDA)
3.  Transformar os dados para modelagem (*Feature Engineering*)
4.  Ajustar e selecionar modelo(s)
5.  Testar projeções
6.  Documentar e comunicar conclusões

**\
\
(1) Definir o objetivo**

Esta etapa não está formalmente definida no processo descrito por [Kuhn e Silge (2023)](#references), porém considero que seja extremamente importante destacar que um processo de modelagem se inicie por uma definição do seu objetivo específico. Sem isso, o cientista de dados arrisca se perder nas etapas seguintes – especialmente na análise exploratória dos dados (EDA).

É especialmente importante definir se o objetivo da modelagem é fazer previsões ([modelagem preditiva]{.underline}) ou explicar relações causais entre as variáveis ([modelagem explicativa]{.underline}). [Shmueli (2010)](#references) descreve as diferenças entre modelos preditivos e modelos explicativos e suas implicações para o processo de modelagem no artigo "*To Explain or to Predict?*". A depender do objetivo da modelagem, diferentes escolhas podem ser feitas, por exemplo, sobre a seleção das variáveis dependentes ou sobre a avaliação da performance do modelo. Em grandes linhas, as etapas do processo de modelagem podem ser aplicadas tanto à modelagem preditiva quanto explicativa, porém a descrição de cada etapa a seguir é voltada para modelagem preditiva, dado o contexto desta disciplina de Modelos Preditivos.

Acredito que possa haver iteração entre a definição do objetivo e o passo seguinte de análise exploratória dos dados (EDA). É possível iniciar-se uma modelagem a partir de um objetivo e alterar ou ajustar o objetivo com base nas descobertas extraídas da análise exploratória de dados. Porém, é importante que os passos seguintes – Feature Engineering, Ajuste e seleção do modelo e Teste das projeções – sejam orientadas pelo objetivo específico predeterminado.

A definição do objetivo deve especificar qual pergunta ou perguntas serão respondidas pelo processo de modelagem.

No caso concreto deste projeto, por exemplo, o objetivo é avaliar a propensão ou risco de rotatividade dos clientes de uma empresa de telecomunicações. O projeto fará uma modelagem para classificação dos clientes entre contratos cancelados ou não cancelados, gerando um modelo final que ajudará a empresa a identificar clientes altamente propensos ao cancelamento de forma que sua área comercial possa atuar especificamente sobre estes clientes. Adicionalmente, gostaríamos de poder reportar a importância relativa das variáveis explicativas do modelo final para identificar os fatores mais influenciadores do risco de cancelamento do cliente.

\
**(2) Analisar os dados (EDA)**

Assumindo-se que o analista esteja partindo de um conjunto pré-selecionado de dados (i.e., desconsiderando-se etapas de desenho de coleta de dados primários ou experimentos), há um processo iterativo entre avaliação e visualização de dados onde diferentes descobertas levam a mais perguntas e talvez buscas paralelas de dados adicionais para obter maior compreensão sobre o problema.

É comum que esta etapa tome mais tempo do que o restante do processo de modelagem. [Wickham, e Grolemund (2023)](#0) contém uma excelente ilustração do processo geral de análise de dados, reproduzida na figura abaixo. A leitura (*Import*) e a limpeza/preparação (*Tidy*) dos dados são mostradas como as etapas iniciais. As etapas analíticas de transformação, visualização e modelagem descritiva geralmente requerem múltiplas iterações antes que se possa, finalmente apresentar os resultados (*Communicate*).

![Processo típico de análise dos dados (Wickham et al. 2023)](.imgs/DataAnalysisProcess.png){alt="Processo típico de análise dos dados (Wickham et al. 2023)" width="400"}\

Deve-se sempre investigar os dados para garantir que eles sejam precisos, adequados e aplicáveis ao objetivo específico da modelagem. A análise exploratória de dados (EDA) pode trazer à tona como as diferentes variáveis estão relacionadas entre si, suas distribuições, intervalos típicos e outros atributos.

Uma boa pergunta a ser feita nesta fase é: "Como obtive esses dados?" Esta pergunta pode ajudar a entender como a amostra foi coletada ou filtrada e se isso foi feito de forma adequada. Existem lacunas nos dados? Essas lacunas são significativas? Qual é a maneira adequada de lidar com essas lacunas? Pode-se excluir as observações com lacunas? Deve-se fazer uma imputação dos dados faltantes? Por técnicas simplistas (e.g. imputação da média, mediana) ou utilizando alguma técnica de modelagem exclusivamente para a imputação das lacunas (e.g. interpolação linear).

Outra boa pergunta a ser respondida nessa etapa é se os dados são relevantes para o objetivo proposto. Qual é a variável de saída/*target*? Quais devem ser suas variáveis explicativas? Deve-se manter todas as variáveis ou selecionar apenas as variáveis com maior potencial de influência nas novas projeções de dados? Há correlações entre as variáveis explicativas? Como isso afeta a modelagem? Essas são algumas perguntas exploratórias que podem começar a ser respondidas durante a etapa de EDA.

Nesta etapa é possível, inclusive, aplicar técnicas de modelagem especificamente para esclarecer a relação entre as variáveis e descobrir a melhor maneira de representar essa relação dentro do modelo preditivo, como por exemplo *Locally Estimated Scatterplot Smoothing* – LOESS.

A etapa de análise de dados, deve estabelecer qual técnica ou técnicas de modelagem serão aplicadas (Regressão Logística, Decision Tree, Random Forest, Redes Neurais, etc. ou mesmo um "*ensemble*" de diferentes técnicas?) para atingir o objetivo determinado.

E finalmente, deve-se definir expectativas claras de como o desempenho (e o sucesso) da modelagem serão avaliados. Pelo menos uma métrica de desempenho deve ser identificada com objetivos realistas do que pode ser alcançado. Os benefícios e desvantagens relativos dessas métricas devem ser ponderados. Também é importante que a métrica seja pertinente – seu alinhamento com os objetivos da análise de dados é crítico.

Nessa etapa também deve-se definir a estratégia de particionamento dos dados a ser adotada para a validação do modelo. Os dados serão separados em treinamento e teste? Se sim, é recomendável que as análises exploratórias sejam feitas sobre o conjunto de dados de treinamento, isolando os dados de teste desde o início da modelagem. Será aplicada alguma metodologia de reamostragem? Como mencionado na pergunta [4.1 Validação cruzada](#validacao-cruzada), pode-se escolher algum tipo de validação cruzada, além de outras metodologias desenhadas para mitigar problemas específicos de tamanho ou distribuição dos dados da amostra de treinamento, como *bootstrapping*.

\
**(3) Feature Engineering**

Os insights de EDA e as decisões tomadas naquela etapa auxiliarão na criação de termos ou componentes de modelo específicos para a modelagem dos dados observados. A transformação dos dados pode incluir metodologias complexas (por exemplo, *Principal Component Analysis - PCA*) ou recursos mais simples (combinando uma medida de altura e outra de largura em uma nova variável da área).

\
**(4) Ajustar e selecionar modelo(s)**

Nessa etapa os modelos são gerados e seu desempenho é comparado. Alguns modelos exigem ajuste de parâmetros em que alguns parâmetros estruturais devem ser especificados ou otimizados.

\
**(5) Testar projeções**

Durante esta fase, as métricas de desempenho do modelo são avaliadas examinando o resíduo ou erro de projeção no conjunto de dados separado para teste. Podem ser feitas outras análises complementares para entender o quão bem o(s) modelo(s) funciona(m).

\
O processo de modelagem é iterativo. Após uma execução inicial dessa sequência de passos, mais entendimento é obtido sobre quais modelos são superiores, bem como quais sub-amostras de dados não estão sendo estimadas efetivamente. Isso leva a EDA adicional e feature engineering, outra rodada de modelagem e assim por diante.

\
**(6) Documentar e comunicar conclusões**

Quando os objetivos originais da modelagem são alcançadas, normalmente as últimas etapas são finalizar, documentar e comunicar o modelo.

\

## XXX Regressão logística

**Treine um modelo de regressão logística para realizar a classificação.**

```{r}
# library(glm)  # Logistic regression model
```

\

**Qual a acurácia, a precisão, a recall e o f1-score do modelo.?**

```{r}

```

\

## XXX Árvore de Decisão

**Treine um modelo de árvore de decisão para realizar a classificação.**

```{r}
# library(rpart)  # Decision Tree 
```

\

**Qual a acurácia, a precisão, a recall e o f1-score do modelo?**

```{r}

```

\

## XXX Random Forest

**Treine um modelo de random forest para realizar a classificação.**

```{r}
# library(ranger) # Random Forest 
```

\

**Qual a acurácia, a precisão, a recall e o f1-score do modelo?**

```{r}

```

\

## XXX Redes Neurais

**Treine um modelo de rede neural rasa para realizar a classificação.**

```{r}
# library(nnet, keras)   # Redes Neurais
```

\

**Qual a acurácia, a precisão, a recall e o f1-score do modelo?**

```{r}

```

\

# XXX Melhor modelo

**Em relação à questão anterior, qual o modelo deveria ser escolhido para uma eventual operação.**

XXXXXXXX

\

# Deep Learning

**Descreva em suas palavras o que é Deep Learning.**

Deep Learning é uma categoria de Machine Learning que usa métodos de Redes Neurais para processar dados de treinamento por meio de camadas de funções não lineares ponderadas (neurônios). Um algoritmo de aprendizado atualiza sucessivamente os parâmetros da rede neural para melhorar a qualidade da estimativa.

Se uma Rede Neural tem muitos neurônios (frequentemente milhões ou mais) e se esses neurônios são organizados em muitas camadas, geralmente com funcionalidades variadas, o algoritmo é chamado de algoritmo de Deep Learning.

Processamento de Linguagem Natural (NLP), reconhecimento avançado de imagem e complementação de código de programação são exemplos de aplicações que usam Deep Learning.

\

# FIM

*Antes de fazer sua entrega, reúna todos os arquivos relativos ao seu Projeto de Disciplina em um único arquivo no formato .zip e poste no Moodle. Utilize o seu nome para nomear o arquivo, identificando também a disciplina, como no exemplo: “nomedoaluno_nomedadisciplina_pd.zip”.*

\

# Referências Bibliográficas {#references}

\
Bruce, P., Bruce, A. e Gedeck, P. (2020). "*Practical Statistics for Data Scientists"* O’reilly Media, Inc. ISBN-13: 978-1492072911.

\
NOT QUOTED: Geisser, S. (1993). *"Predictive Inference: An Introduction."* Chapman and Hall, Londres. DOI: 10.1201/9780203742310. [[online](https://doi.org/10.1201/9780203742310)].

\
Kuhn, M. e Silge, J. (2023). "*Tidy Modeling with R: A Framework for Modeling in the Tidyverse*". O’reilly Media, Inc. ISBN-13: 978-1492096450 [[online](https://www.tmwr.org/)]

\
Shmueli, G. (2010). "*To Explain or to Predict?*" Statistical Science, Institute of Mathematical Statistics [online] v. 25, n. 3, p. 289–310. DOI: 10.1214/10-STS330. [[online](https://projecteuclid.org/euclid.ss/1294167961)]

\
Wickham, H., Çetinkaya-Rundel, M. e Grolemund, G. (2023). "*R for Data Science, 2nd Edition*". O'Reilly Media, Inc. ISBN-13: 978-1492097365. [[online](https://r4ds.hadley.nz/)]

\

------------------------------------------------------------------------
